---
title: "BMIN503/EPID600 Project: Exploring significant predictors for decision of taking PrEP among high-risk population"
author: "Weilu Song"
output: 
  html_document:
    theme: paper 
    highlight: tango
---


***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers. Feel free to change the theme and other display settings, but this is not required.

### Overview
To better control HIV infection among high risk population, it urgently needs to explore the relationship between related determinants that may affect Pre-Exposure Prophylaxis (PrEP) usage. Jilinde PrEP cohort study was conducted in Kenya, it contains over 1000 participants and over 300 variables that related to PrEP usage among high risk population. In this project, curated dataset is used to analyze the potential PrEP predictors that may impact the PrEP usage among targeted population.\



The original files of this final project can be accessed from here: [Final project_Song](https://github.com/aaronwlsong/BMIN503_Final_Project) \
 

### Introduction 
Pre-Exposure Prophylaxis (PrEP) has been proven an effective way to prevent HIV infection.However, the PrEP coverage among high risk population is still low. The reason behind it is complex, because making a decision of taking PrEP is determined by multiple factors such as social determinants, self-perception, and clinical history. To better curb HIV infection among high risk people, it is urgent to identify the key factors that may affect the decision of taking PrEP. Becasue that will tremendously helps both researcher and local policy makers to advocate PrEP as a main HIV prevention strategy in Kenya.\
\
As aforementioned, because the factors that may influence the decision of taking PrEP are interdisciplinary, when conduct and collect the dataset, the researchers always try to capture as much participant's characteristic as possible. Therefore, it leads a results that a typical epidemiological dataset contains over 100 or sometimes couple hundreds variables and some of those variables are highly collinearity. As a well known secret, a precise result is highly depended on model and variable selection, once the outcome is settled, the main challenges for researchers are how to find the significant associated predictors related to outcome and establish the best predictive model to answer the hypothesis they posed.\
\
In recent decade, machine learning method has been widely appided in more and more field, it also shapes the way how to establish a predictive model in the field of public health. Compared to the traditional way which the variables/predictors were selected by heavily relying on the researcher's background knowledge and literature review, using the best fitted model, it not only helps researchers to quickly pick up the most significant associated variables among the hundreds variables, but also enables them to optimize the model based on the result to eventually get a satisfied predictive result. \
\
In this study, we use this JILINDE cohort study, but only the baseline completed dataset to explore the significant predictor for decision of taking PrEP, by applying the dataset into multiple supervised machine learning models to find the most significant predictor, from there, the research also demonstrate how to applied causal inferential theory on the selected variables and model to further optimize it to maximize its AUC value. We expect to through this research, it can provide practical references for HIV prevention through PrEP in local area, moreover, present a plausible way to optimize model by integrating causal inferential theory to machine learning model. \ 
\

##### Hypothsis
Given the introduction, in this research, the **primary aim** is exploring significant predictors for outcome: Decision of taking PrEP (original variable name: agree_prep). And the **secondary aim** is comparing different machine learning model's performances given a imbalanced dataset.\

              
### Methods
\
Given the consideration of workload of missing data imputation, complication of analysis of repeated measure dataset and etc, in this project, we curate the dataset by only using the baseline data for the final analysis. In addition, in data cleaning process, instead of imputing the missing data, we remove the observations with missing data, using the completed dataset for analysis. \
\
**Step 1: Outcome variable inspection** Fist, we pick up a outcome variable based on primary study hypothesis. The outcome variable is agree_prep, which has the definition of whether agree to take PrEP. barplot is used to inspect the outcome variable, to see whether it has balanced outcomes across yes and no group. \
\
**Step 2: Demographic variable inspection** Second, we use the map tool,barplot and violin plot to explore the single relationship between each demographic variable and outcome. For example, map tool shows the distribution of study sites given the gross income and age in Kenya. Then the Table 1 is used to present the overall results. \  
\
**Step 3: Variable selection** Third, Then we used variables (other than outcome vairiable) as predictors to find the statistical significant predictors that are related to outcome at $\alpha$=0.05 level. This process is firstly used GLM model, and then use the random forest tree to replicate the process and collect top significant predictors based on the Gini score result. We list the Gini score result from the RF and P-value from GLM side by side to highlighted how many variables are overlapped in these two methods. \
\
**Step 4: Model evaluation** Fourth, because of imbalanced dataset we used in this research, to compare and choose the best model to handle this type of dataset, we compare 3 machine learning models: SVM, Random forest tree and GLM, then use k-cross validation (k=10), ROC curve and AUC to evaluate each model's performance.\
\

**Step 5: Create original GLM model**\
1) The original GLM model. The original model will be use the top 6 statistical significant variables/predictors that are selected by GLM, meanwhile, these 6 variables must be also showed in Gini score result.\ 
\

**Step 6: Optimize GLM model**
1) Optimize variables in model. We will re-evaluate the model and its independent variables to see which one should be removed. The causal inferential and DAG theory are borrowed in this process, even though this is a cross-sectional study and we do not specify the exposure. But we treat the all selected variables in original GLM model as a bundle of exposure, then evaluate whether the demographic variables with statistical significant among yes and no groups in table 1 should be included. The criteria for this decision step is that whether this variable plays the role like potential confunding. Similarily, the excluded rule for this step is that if the selected variable is more like Effect Measure Modification (EMM), we will remove it from the model. \
\
2) Generate the results. The final model after optimized its dependent variables will be presented in Table 2. \
\
3) Compare the model performance. The last step in this section is evaluation and comparision the original GLM model and optimized GLM model.We re-rum the K-cross validation (k=10),  ROC curves as well as AUC to explore whether after optimization, it helps to improve the performance of the predictive model. \
\
The flow chart about the method is presented as follow: \

![Figure 1. Flowchart of research method](/Users/weilus-adm/Desktop/BMIN5030/4_Final_project/BMI5030_FP_Song/method.png)



#### 1. Data preparation
```{r message=F, warning=F}
pacman::p_load(rio,here,skimr,tidyverse,dplyr,haven,codebook,knitr,BiocManager,rlist,tools,qdapTools,janitor,fastDummies,labelled,magrittr,plyr,readxl,data.table,purrr,stringr,readxl,tidyr,dplyr,fastDummies,questionr,sqldf, fs, kableExtra, lubridate, mise,mice, VIM,mitools, sjmisc,randomForest,pROC,PRROC,glmnet, factoextra, ggdendro, Rtsen,devtools,e1071,GEOS,gtsummary,dagitty,lavaan)
library(devtools)
install_github("vqv/ggbiplot")
```
\
##### 1-1 Data loading and cleaning
```{r message=F, warning=F}
#rm(list=ls())
# Retreive baseline data from cohort study
df503<-rio::import(here::here("rawdata","jilinde_coh_2022aug.csv"))%>%
  dplyr::filter(event_name=="Baseline")

# Remove the missing value and only one unique value. 
dfcheck<-skimr::skim(df503)%>%
  tibble::as.tibble()%>%
  dplyr::filter(character.empty==0)%>% # Keep completed case
  dplyr::filter(character.n_unique >1) #Remove only one level factor

#Convert categorical variables as factor and derived age group: agegrp based on continous var: age
fdat<-df503%>%
   dplyr::select(dfcheck$skim_variable, agree_prep, age)%>%
   dplyr::mutate(age=as.numeric(age),
          agegrp=dplyr::case_when(age<25|age==25~"Young",
                                   age>25~"Adult",))%>%
   dplyr::filter(agree_prep!="")%>% #Remove the null value in outcome
   dplyr::select(everything(), -c(age,data_access_group, record_id, record_create_date))%>%
   dplyr::mutate_each_(dplyr::funs(factor(.)),names(dfcheck$skim_variable)) #remove unused variable 

cols<-names(fdat)[!(names(fdat) %in% c("age"))]
 fdat<-fdat%>%dplyr::mutate_each_(dplyr::funs(factor(.)),cols) #convert to factors

#str(fdat) 
table(fdat$agree_prep)
#colSums(is.na(fdat))
```

#### 2. Dependent/outcome variable inspection

```{r message=F, warning=F}
ggplot(fdat, aes(factor(agree_prep), fill= agree_prep))+
  geom_bar()+
  xlab("Agree to take PrEP")+
  geom_text( stat = "count", aes(label = ..count..), vjust = 1.5, colour = "black")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()
```
\
As we can see from the bar plot, in this dataset, the outcome for this research is extremely imbalanced. The participants who would like to take PrEP are 1108, whereas only 24 participants chose No.This extreme imbalanced dataset will affect machine learning algorithm in a negative way.\

\
##### 2-1. Map viz 
In this part, we would like to use map tool to visualize the dataset to see the participant's locations of this study given the backgound of nationwide distribution of age and gross income separately.\

```{r message=FALSE, warning=FALSE}
library(tigris)
library(ggplot2)
library(sf)
library(maptools)
library(raster)
library(plyr)
library(rgdal)
library(geodata)
library(RColorBrewer)
library(cowplot)

dfmap<-df503%>%
    dplyr::select(event_name,longitude,latitude, agree_prep,county,gross_income, age, sex, education_level,employ_status,marriage_status)%>%
    dplyr::filter(event_name=="Baseline")%>%
    dplyr::filter(gross_income!="Unknown")%>%
    dplyr::filter(sex!="Other")%>%
    dplyr::filter(marriage_status!="")%>%
    tidyr::fill(longitude, .direction = 'down')%>%
    tidyr::fill(latitude, .direction = 'down')%>%
     mutate(age=as.numeric(age),
          agegrp=dplyr::case_when(age<25|age==25~"Young",
                                  age>25        ~"Adult",))%>%
   dplyr::filter(agree_prep!="") 

dfmap2<-dfmap%>%dplyr::select(longitude,latitude, agree_prep,gross_income, age,sex, education_level,employ_status,marriage_status, agegrp)
sfmap<-st_as_sf(dfmap2, coords = c("longitude", "latitude"), crs = 4326)

Kenya<-getData("GADM", country="KE", level=0)
Kenya1<-getData("GADM", country="KE", level=1)

ke<-st_as_sf(Kenya1)

ke2<-ke%>%
  st_join( sfmap, ke,join = st_nearest_feature, left = T) 

sherrie_theme <- function() {
  theme_minimal() +                                  # shorthand for white background color
  theme(axis.line = element_blank(),                 # further customization of theme components
        axis.text = element_blank(),                 # remove x and y axis text and labels
        axis.title = element_blank(),
        panel.grid = element_line(color = "white"),  # make grid lines invisible
        legend.key.size = unit(0.8, "cm"),           # increase size of legend
        legend.text = element_text(size = 9),       # increase legend text size
        legend.title = element_text(size = 9),
        plot.title = element_text(size=12, hjust = 0.5))      # increase legend title size
}
myPalette <- colorRampPalette(brewer.pal(9, "BuPu"))

#Create the first plot with distribution of gross_income
p1<-ggplot() + 
  geom_sf(data = ke2, aes(fill=gross_income)) +
  geom_sf(data =sfmap, color = "red")+
  sherrie_theme()+
  ggtitle("Study conducted place \n given the distribution \n of gross income in Kenya") + # add plot title
  scale_fill_gradientn(name = "Gross income \n in local currency",      # change legend title
                    colours = myPalette(100))

#Create the first plot with distribution of age

myPalette2 <- colorRampPalette(brewer.pal(9, "YlOrBr"))
p2<-ggplot() + 
  geom_sf(data = ke2, aes(fill=age)) +
  geom_sf(data =sfmap, color = "navyblue")+
  sherrie_theme()+
  ggtitle("Study conducted place \n given the distribution of age in Kenya") + # add plot title
  scale_fill_gradientn(name = "Age \n (year)",      # change legend title
                    colours = myPalette2(100))
plot_grid(p1,p2, labels="AUTO") #Put the two plots together

```
\
From the map plots, we can see one geographic location is out of Kenya, which should be suspected as outlier and need to check with data provider, and most sampled participants are in southern of Kenya, this will limited the generalization when we want to use the result to prdict participant in other regions of Kenya. Other than that, when eyeball these two plot, it looks like all samples are randomly across the Kenya, in other words, there is no obvious sign that all sampled participants are all from rich area or poverty area, neither for the variable of age (Second plot).\
\

##### 2-2. Single demographic variables visualization
In addition to map the study, we also want to visualize the relationship between all demographic variables and outcome variable: agree_prep. \

```{r message=FALSE, warning=FALSE}
#First: continuous variables 

# Outcome and age
pa<-ggplot(dfmap2, aes(x=factor(agree_prep), y=age, fill=agree_prep))+
  geom_violin(width=1.4)+
  geom_boxplot(width=0.1, color="navyblue", alpha=0.2)+
  xlab("Agree to take PrEP")+
  ylab("Age (year)")+
  ggtitle("Outcome and age")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()

# Outcome and gross income 
pb<-ggplot(dfmap2, aes(x=factor(agree_prep), y=gross_income, fill= agree_prep))+
  geom_violin(width=1.4)+
  geom_boxplot(width=0.1, color="navyblue", alpha=0.2)+
  xlab("Agree to take PrEP")+
  ylab("Age (year)")+
  ggtitle("Outcome and gross income")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()
plot_grid(pa,pb, labels="AUTO")

#Second: Categorical variables

# Outcome and sex 
library(ggmosaic)
pc<-ggplot(dfmap2,aes(x=sex,y=agree_prep,fill= agree_prep))+
  geom_col()+
  xlab("Sex")+
  ylab("Agree to take PrEP")+
  #ggtitle("Outcome and gross income")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()+
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Outcome and education_level
pd<-ggplot(dfmap2)+
  geom_col(aes(x=education_level, y=agree_prep, fill= agree_prep))+
  xlab("Educational Level")+
  ylab("Agree to take PrEP")+
  #ggtitle("Outcome and education level")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()+
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Outcome and employ_status
pe<-ggplot(dfmap2)+
  geom_col(aes(x=employ_status, y=agree_prep, fill=agree_prep))+
  xlab("Employ Status")+
  ylab("Agree to take PrEP")+
  #ggtitle("Outcome and education level")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()+
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# Outcome and marriage_status
pf<-ggplot(dfmap2)+
  geom_col(aes(x=marriage_status, y=agree_prep, fill= agree_prep))+
  xlab("Marriage Status")+
  ylab("Agree to take PrEP")+
  #ggtitle("Outcome and education level")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

pg<-ggplot(dfmap2)+
  geom_col(aes(x=agegrp, y=agree_prep, fill= agree_prep))+
  xlab("agegrp")+
  ylab("Agree to take PrEP")+
  #ggtitle("Outcome and education level")+
  scale_fill_brewer(palette = "Pastel1")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
  
plot_grid(pc, pd, pe,pf)
plot_grid(pg, 2,1 )
```

\

From the plots, we can tell the distribution for each demographic variable between "Yes" and "No"group. However, we need to more specific information to see whether those varied distributions/frequencies have significant different given $\alpha$=0.05.To get that table we use gtsummary to generate table 1 in next step.
\

##### 2-3. Create Table 1: Demographic description of study

```{r}
library(gtsummary)
library(tidyverse)
dfmap%>%
  dplyr::select(agree_prep,gross_income, age,agegrp,sex, education_level,employ_status,marriage_status)%>% #select and adjust variable's display order
  dplyr::mutate(age=as.numeric(age))%>%
  gtsummary::tbl_summary(
    by=agree_prep,
      statistic = list(all_continuous() ~ "{mean} ({sd})",
                       all_categorical() ~ "{n} / {N} ({p}%)"),
    label=list(age="Age",sex="Sex", gross_income="Gross income", education_level="Education",employ_status="Employee",marriage_status="Marriage")
  )%>%
  add_p()%>%
  add_overall()%>%
  modify_header(label ~ "**Variable**")%>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Whether agree to take PrEP**")%>%
  modify_caption("**Table 1. Demographic description of study**") %>%
  italicize_labels()%>%
  bold_labels()

```
\
Based on the table 1, we can see most of demographic variables are not significantly different among two groups (Agree to take PrEP v.s Not agree to take PrEP). The significant different might be resulted by imbalanced participants in two groups.\
\

#### 3.Using Machine learning method to explore variables 
Because the dataset (fdat) has over 100 variables. It is impossible to manually find significant associated variable to outcome.Since the outcome is binary, we will use GLM model to detect all significant associated variables to outcome given the $\alpha$=0.05 level. \
\

##### 3-1. Select significant associated variables by using GLM
```{r message=F, warning=F}
# Finding a significant associated variables by using glm model.
datsig<-fdat%>%
  tidyr::pivot_longer(cols = -c(agree_prep)) %>%
  tidyr::nest(data = -name) %>%
  mutate(mod = purrr::map(data, ~glm(agree_prep~value, data = .x, family = binomial())),
         p.value = purrr::map_dbl(mod, ~summary(.x)$coefficients[2,4])) %>%
  dplyr::filter(p.value < 0.05)%>%
  dplyr::select(Var_Name=name, `P-value`=p.value)%>%
  arrange(`P-value`)  
  #mutate(No.=seq_along(name))%>%
  kableExtra::kbl(datsig, caption = "Table 2. Variables that significant associated with outcome: by GLM algorithm ")%>%
    kableExtra::kable_classic(full_width=F, html_font = "Cambria") 
```
\
The table lists all variables that are significant associated with outcome. However, we also want to know whether the other machine learning algorithms perform better than GLM. If so, we can choose the others to predict the outcome. Here, we also used Random Forest tree and SVM as comparison. \
\

#### 3-2. Replicate the variable selection process by using other ML models: Random forest tree and SVM.
```{r message=F, warning=F}
# Random forest tree: Top 19 dfgini
library(randomForest)
df.rf <- randomForest(agree_prep ~ ., data =fdat, ntree = 200, importance = TRUE)
df.rf
#df.rf$importance
df.gini<-sort(df.rf$importance[,4], decreasing =T)
dfgini<-head(df.gini,19)
  kbl(dfgini, caption="Table 3. Top 19 predictors and their gini scores: by Random forest tree algorithm", digits = 3)%>%
    kable_classic(full_width=F, html_font = "Cambria") 

# SVM
library(e1071)
fdat.svm <- svm(agree_prep ~ ., data = fdat, scale = TRUE, kernel ="radial")
svm.pred <- fitted(fdat.svm)
table(fdat$agree_prep, svm.pred) 
```

\
Given the results from RF and SVM, we can see that RF performs better than SVM since it can correctly predict 9 negative results whereas the SVM can correctly predict ZERO. However, both of them have unsatisfied performances due to extreme imbalanced dataset. \

\
Hence, we only use Gini score from RF as reference by creating a side by side list to present both top 19 variables/predictors which are significant associated with outcome. \

#### 3-3. Create the significant associated variable list based on Random forest and GLM model 

```{r message=F, warning=F}
# Present the two selected results:
kbl(cbind(dfgini, datsig),caption="List 1.  Comparison of top-ranked significant varaibles by two algorithms", digits = 3)%>%
    add_header_above(c("Random Forest Model"= 2, "GLM Model" = 2))%>%
   kable_classic(full_width=F, html_font = "Cambria")
```
\

The next step is visualizing the performance for each model we introduced before (GLM, RF and SVM).  We use K-cross validation (K=10) to see whether the assumption we made before (Extreme imbalanced dataset will impact the ML performance in a negative way) is correct based on the results of ROC curve and AUC.Then, we will pick up the best performance model and variables/predictors from the list we created in previous step (3-3) to construct our final model. \
\

#### 4. Model selection and evaluation
##### 4-1 Using Machine learning method to evaluate model's performance and select a model
```{r message=F, warning=F}
N = nrow(fdat)
K = 10
set.seed(1234)
s = sample(1:K, size = N, replace = T)
pred.outputs.svm<- vector(mode = "numeric", length = N)
pred.outputs.glm<- vector(mode = "numeric", length = N)
pred.outputs.rf <- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
    train <- dplyr::filter(fdat, s != i)
    test <- dplyr::filter(fdat, s == i)
    obs.outputs[1:length(s[s == i]) + offset] <- test$agree_prep

    #SVM train/test
    svm.m <- e1071::svm(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never, data = train, scale = TRUE, 
                 kernel = "radial", probability = TRUE)
    svm.pred.curr <- predict(svm.m, test, probability = TRUE) 
    pred.outputs.svm[1:length(s[s == i]) + offset] <- attr(svm.pred.curr, "probabilities")[ , 1]
    
  #GLM train/test
    df.glm<-glm(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+service_received_msm_condoms+risk_low_known+risk_low_condoms, data =fdat, family = binomial())
    glm.pred.curr <- predict(df.glm, newdata=test, type="response") 
    pred.outputs.glm[1:length(s[s == i]) + offset] <- glm.pred.curr
    
    
    #RF train/test
    rf <- randomForest::randomForest(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never, data = train, ntree = 100) 
    rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
    pred.outputs.rf[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
    
    offset <- offset + length(s[s == i])
}
library(pROC)
plot.roc(obs.outputs, pred.outputs.svm, percent = TRUE,  print.auc= TRUE, print.auc.y = 30,ci = TRUE, col = "blue")
plot.roc(obs.outputs, pred.outputs.rf, percent = TRUE,  print.auc= TRUE, print.auc.y = 40,ci = TRUE, col = "darkgreen", add = TRUE)
plot.roc(obs.outputs, pred.outputs.glm, percent = TRUE,  print.auc= TRUE,print.auc.y = 50,ci = TRUE, col = "red", add = TRUE)
legend("bottomright", legend = c("SVM", "Random forest tree","GLM"), col = c("blue", "darkgreen","red"), lwd = 2, cex=.7)
```
\
The result shows that our two assumption is correct. First, the extreme imbalanced dataset indeed lead to very unsatisfied predicted performance for each model (None of them has AUC over 80%). In addition, among the three models, the SVM has the worst predicted performance among three ML models, which is consistent of statistics from each model.\

\

##### 4-2. Create original GlM Model based on top 6 significant variables 

Since the GLM has better performance to handle this imbalanced dataset than others, we will use GLM model and the top 6 variables from the GLM calcuation which are also appear in RF's Gini result. (See list in 3-3) .\

```{r message=F, warning=F}
glmfdat<-glm(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+service_received_msm_condoms+risk_low_known+risk_low_condoms, data =fdat, family = binomial())
s<-summary(glmfdat)
sdf<-as.data.frame(s$coefficients) 
z<-exp(coef(glm(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never, data=fdat, family = binomial()))) 

z<-as.data.frame(round(z,3))%>%
  dplyr::select(Exp_coeff="round(z, 3)")
  kableExtra::kbl(cbind(sdf,z), caption="Table 4. Summary of original GLM model", digits = 3)%>%
    kableExtra::kable_classic(full_width=F, html_font = "Cambria")
```
\

##### 4-3. Optimize predictive model

Because we already know the extreme imbalanced dataset can cause unreliable statistic result. We will re-evaluate the all included variables/predictors, the goal in this process is that we want to improve the performance given the current model. It is a subjective process, which needs to do three things: One is re-consider the top variables generated by Gini score (Table 2), to see whether we should include them into model; Second, we will calculate the collinerity among the current selected variables; The third step is borrowing concept of DAG theory to see whether demographic variables should be added to increase the performance of predictive model.\

1). **Re-consider top variables generated by gini score*.** \

According to the Gini score result, we see several variables that have prefix of "decline" have significant association with outcome. But we can tell that the common characteristic of these top predictors generated by RF is that all of them have extreme imbalanced outcome number:  the number for "yes" is less 10. For example, the top 1 variable: decline_adhere, the number of "yes" is only 9 out of 1123. \

Whereas, the GLM consider more balanced outcome variables, the top 6 we included in model are relative balanced compare to the top 6 variables in Gini score(table 3), which all variable's  "yes" number is over 25. That is why we should include top 6 variables from GLM instead of RF. \So we adhere to the GLM models result and NOT adding any of these variables in to current model.\
\

```{r}
#Compare top predictor generated by gini score and GLM. Exclude critieria: remove extreme imbalanced variable from model, such as outcome is less 10 obs.
table (fdat$decline_adhere) #Top 1 in Gini score
table (fdat$decline_other) #Top 2 in Gini score
table (fdat$decline_risk) #Top 3 in Gini score
table (fdat$decline_family) #Top 4 in Gini score
table(fdat$decline_decrease) #Top 5 in Gini score
table(fdat$decline_test) #Top 6 in Gini score
table(fdat$service_received_msm_risk) #Top 1 in GLM
table(fdat$service_received_msm_hiv) #Top 2 in GLM
table(fdat$service_received_msm_condoms)#Top 3 in GLM
table(fdat$risk_low_known)#Top 4in GLM
table(fdat$identity_group_msm) #Top 5 in GLM
table(fdat$risk_low_condoms)#Top 6 in GLM

```
\
2). **Using DAG conception to add demographic variables*.** \
According to the table 1 result, we found that only sex and age group (agegrp) have a statistical significant among different groups. So we introduce DAG conception to see which one or whether both of them should be included in to the model if these variables affect both predictors and outcome. \
```{r}
library(dagitty)
library(lavaan)
g <- dagitty('dag {
    Current_selected_vars [pos="0,1"]
    Outcome  [pos="2,1"]
    Sex      [pos="0.5,0"]
    Agegroup [pos="1,0"]
    
    Current_selected_vars-> Outcome
    Current_selected_vars <- Agegroup-> Outcome
    Agegroup-> Outcome
    Current_selected_vars<-Sex
}')
plot(g)
```
According to the DAG, we can see that agegroup should be included into the model becasue it affects both current selected vars and outcome given the background knowledge. However, even though the sex shows statistical different in table 1, we do not need to include it becasue the sex may affect all current selected variables, however, it does not affect the outcome (decision of taking PrEP). In other words, there is no evidence that the women has siginifcant willingness to take PrEP compared to men or vice versa. \

3). **Collinearity checking before and after optimize model** \

```{r}
glmfdat2<-glm(agree_prep~ service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never+agegrp, data =fdat, family = binomial())
a<-car::vif(glmfdat)
kableExtra::kable (a, caption="VIF for variables in original GLM model", digits = 3)%>%
    kableExtra::kable_classic(full_width=F, html_font = "Cambria")
b<-car::vif(glmfdat2)
kableExtra::kable (b,caption="VIF for variables in optimized GLM model", digits = 3)%>%
    kableExtra::kable_classic(full_width=F, html_font = "Cambria")
```
\
In this process, we found the variable "identity_group_msm", "service_recived_msm_hiv", "service_received_msm_risk" have a high collinearity: their VIF are close or higher than 5. So we should remove some of these variables to eliminate the Col linearity. Because based on background knowledge, msm received service usually contains HIV prevention service, so we removed the variable of service_received_msm_hiv, similarly reason, we should remove identity_group_msm.\

After adjusted, we can see the variables VIF decrease to under 2.This implies that we successfully fix collinearity issue in original model and selected variabls before.\ 
\
So, our final selected predictive variables are: \

|**Variables in original GLM**|**Variables in optimized GLM**|**Outcome/dependent variable**| 
|:--:|:---:|:---:|
|service_received_msm_hiv | |agree_prep|
|service_received_msm_risk |service_received_msm_risk| |
|risk_low_known|risk_low_known| |
|risk_low_condoms|risk_low_condoms| |
|risk_low_never|risk_low_never| |
|identity_group_msm| | |
| |agegrp| |

\

##### 4-4. Create optimized GLM model
```{r message=F, warning=F}
  glmfdat2<-glm(agree_prep~ service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never+agegrp, data =fdat, family = binomial())
  s2<-summary( glmfdat2)
  sdf2<-as.data.frame(s2$coefficients) 
  z2<-exp(coef(glm(agree_prep~service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never+agegrp, data=fdat, family = binomial()))) 
  z2<-as.data.frame(round(z2,3))%>%
  dplyr::select(Exp_coeff="round(z2, 3)")
  kableExtra::kbl(cbind(sdf2,z2), caption="", digits = 3)%>%
    kableExtra::kable_classic(full_width=F, html_font = "Cambria")
```

##### 4-5. Create the Table 2: Statistics of optimized GLM model
```{r}
glmfdat2%>%
  tbl_regression(exponentiate=T, 
                 label=list(service_received_msm_risk="Whether received MSM risk education service",
                            risk_low_known="Have known HIV status of all sex partner(s)",
                            risk_low_condoms="I consistently use condoms",risk_low_never~"Never had sex",
                            service_received_msm_peer="Whether support by peer groups",
                            agegrp="Age group"))%>%
  modify_table_body(~.x %>% 
      mutate(label  = ifelse(label == "Checked", "Yes",
                             ifelse(label =="Unchecked", "No",label))))%>%
  bold_p()%>%
  modify_header(label ~ "**Variable**")%>%
  modify_caption("**Table 2. Statistics of optimized model**") %>%
  italicize_labels()%>%
  bold_labels()
```
The result shows that most of variables have statistical significant association to outcome.Those variables can also be used as a good predictors to predict whether the target population are willing to take PrEP or not. Even though we can see strong magnitude for certain variables such as var: Never had sex (OR=18.1), we also notice that the 95% CI is extremely wide, some of them also across the 1, which indicates the statistical result is not precise. This is expected output given the fact that extreme unbalanced number of outcome aforementioned. \

\
#### 4-6. Compare two models'performance
The last step for this project is reviewing the performance of the optimized model. By comparing to original model, which was only selected the top 6 variables (overlapped with Gini Score result) from GLM result, we use ROC curve and AUC value to see whehter the optimized model improve the rationality and predictive precision. \

```{r message=F, warning=F}
N = nrow(fdat)
K = 10
set.seed(1234)
s = sample(1:K, size = N, replace = T)
pred.outputs.glm<- vector(mode = "numeric", length = N)
pred.outputs.glm2<- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
    train <- dplyr::filter(fdat, s != i)
    test <- dplyr::filter(fdat, s == i)
    obs.outputs[1:length(s[s == i]) + offset] <- test$agree_prep

    
  #GLM train/test: First model with top 6 significant predictors in GLM model
    df.glm<-glm(agree_prep~identity_group_msm+service_received_msm_hiv+service_received_msm_risk+service_received_msm_condoms+risk_low_known+risk_low_condoms, data =fdat, family = binomial())
    glm.pred.curr <- predict(df.glm, newdata=test, type="response") 
    pred.outputs.glm[1:length(s[s == i]) + offset] <- glm.pred.curr
    
  #GLM train/test: Second model with adjusted variables based on causal inferential theory
        df.glm2<-glm(agree_prep~service_received_msm_risk+risk_low_known+risk_low_condoms+risk_low_never+agegrp, data =fdat, family = binomial())
    glm.pred.curr2 <- predict(df.glm2, newdata=test, type="response") 
    pred.outputs.glm2[1:length(s[s == i]) + offset] <- glm.pred.curr2
    
   offset <- offset + length(s[s == i])
}
library(pROC)
plot.roc(obs.outputs, pred.outputs.glm, percent = TRUE,  print.auc= TRUE, ci = TRUE, col = "red")
plot.roc(obs.outputs, pred.outputs.glm2, percent = TRUE, print.auc = TRUE, print.auc.y = 40,ci = TRUE, col = "navyblue", add = TRUE)
legend("bottomright", legend = c("GLM: Unadjusted variables","GLM: Adjusted variables by DAG"), col = c("red","navyblue"), lwd =2, cex=.7)
```
\
From the plot, we can see that the optimized the model increases the precision of perdition. The AUC value is increased from 78.2% to 82.4%. This means that adding variable of age group helps to improve the models performance. \

\

### Results 
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.\
There are couple of finding based on the this research:\
\
1)**Main result** \
The result shows that in optimized model, 4 variables out of 6 have statistical significant association with outcome. They are:Whether received MSM risk education service(service_received_msm_risk, p-value=0.038, OR=13.5), I consistently use condoms(risk_low_condoms, p-value=0.001, OR=7.15),Never had sex(risk_low_never, p-value=0.013, OR=18.1) and Age group (agegrp, P-value=0.016, OR=0.18).\

To interpret this result, we can say that the people who are MSM and have not received the risk education show stronger willingness to take PrEP, this result is inconsistent with previous finding, the further research should be carried to explore the reason. Other than that, the people who are older, have had sex before, and who do not consistently use condom are more likely to consider taking PrEP. Those findings are consistent with previous studies.\

The result also shows the practical meaning, that reveals the target population and key factors regard to PrEP usage in Kenya if the local government want to advocate it as the prevention for HIV. \
\
2) **Variable selection** \
The Jilinde study has total 4115 obs. and 367 variables. After data cleaning, the final dataset downsized to 1132 observations and 106 variables. Among all of these variables, 19 variables are selected by GLM model as the significant predictors for outcome. And most of the variables are consistent with the result of Gini score result. In other words, they are overlapped with Random forest tree's result. \
The outcome inspection result shows that the dependent variable has extreme unbalanced outcomes (24 v.s 1108).This severely affects the model's performance. \
\
3) **Model selection and evaluation** \
The study compared SVM, Random forest tree and GLM for the predictive performance.The result shows that GLM has the best performance for this extreme unbalanced number outcome. The worst performance is SVM model, which can not correctly predict for small number of outcome (correctness is zero). \

The GLM and top 6 significant predictors are used as original predictive model, the AUC value and ROC result shows it only has relative good performance (AUC is about 78.2%). After we introduced the DAG theory to the model optimization, we removed some variables which are suspected as EMM and added some demographic variables. The final GLM model shows that this process improves the model's performance and promote rationality, comparing the original GLM model, the AUC value is increased to 82.4%. \
\ 

### Discussion

1) **Unbalanced number of outcome** \
Given the fact that this project used the imbalanced data, it proves that machine learning models usually cannot perfectly handle that. Specifically. Then imblalanced dataset here means that a classification data set with skewed class proportions. According to the previous study, machine learning models will encounter a problem for such type of dataset because when it trains on unbalanced datasets, it often have poor results when they have to generalize (predict a class or classify unseen observations). Moreover, some models will be more susceptible to unbalanced data than others despite of the chosen algorithm.[1] This has been proved in this project as we found in model selection section. To make this result more meaningful in practice, further some advanced methods such as random oversampling should be introduced to fix it.[2] \
\
2) **Completed case**\
This study used the completed cases to implement the analysis, which may casue biases since the missing value is highly not lost randomly. In practice, we should consider to adopt the imputation package in R to fix the missing value issue if the missing proportion is under the certain percentage, say less than 30%. Morever, it implies the importance for researchers about the quality of dataset when they design and collect the dataset.[3][4] \


### Reference
[1] Cieslak, D. A., & Chawla, N. V. (2008, September). Learning decision trees for unbalanced data. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 241-256). Springer, Berlin, Heidelberg. \
[2] Viloria, A., Lezama, O. B. P., & Mercado-Caruzo, N. (2020). Unbalanced data processing using oversampling: Machine learning. Procedia Computer Science, 175, 108-113. \
[3]Gorelick, M. H. (2006). Bias arising from missing data in predictive models. Journal of clinical epidemiology, 59(10), 1115-1123.\
[4]White, I. R., & Carlin, J. B. (2010). Bias and efficiency of multiple imputation compared with complete‐case analysis for missing covariate values. Statistics in medicine, 29(28), 2920-2931.\
